{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers --upgrade","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-30T10:22:24.563005Z","iopub.execute_input":"2024-06-30T10:22:24.563295Z","iopub.status.idle":"2024-06-30T10:22:49.440348Z","shell.execute_reply.started":"2024-06-30T10:22:24.563269Z","shell.execute_reply":"2024-06-30T10:22:49.439378Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nCollecting transformers\n  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\nSuccessfully installed transformers-4.42.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pypdf2","metadata":{"execution":{"iopub.status.busy":"2024-06-30T10:22:49.442313Z","iopub.execute_input":"2024-06-30T10:22:49.442661Z","iopub.status.idle":"2024-06-30T10:23:02.673131Z","shell.execute_reply.started":"2024-06-30T10:22:49.442628Z","shell.execute_reply":"2024-06-30T10:23:02.672029Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pypdf2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pypdf2\nSuccessfully installed pypdf2-3.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install PyPDF2 pymupdf nltk sentence-transformers scikit-learn faiss-cpu transformers langchain bitsandbytes\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T10:23:02.674604Z","iopub.execute_input":"2024-06-30T10:23:02.674912Z","iopub.status.idle":"2024-06-30T10:23:26.881610Z","shell.execute_reply.started":"2024-06-30T10:23:02.674882Z","shell.execute_reply":"2024-06-30T10:23:26.880473Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: PyPDF2 in /opt/conda/lib/python3.10/site-packages (3.0.1)\nCollecting pymupdf\n  Downloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting langchain\n  Downloading langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nCollecting PyMuPDFb==1.24.6 (from pymupdf)\n  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.23.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.10 (from langchain)\n  Downloading langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\nCollecting packaging (from faiss-cpu)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (2.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.2.6-py3-none-any.whl (975 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.10-py3-none-any.whl (332 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.82-py3-none-any.whl (127 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: PyMuPDFb, packaging, orjson, pymupdf, faiss-cpu, langsmith, bitsandbytes, langchain-core, sentence-transformers, langchain-text-splitters, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyMuPDFb-1.24.6 bitsandbytes-0.43.1 faiss-cpu-1.8.0.post1 langchain-0.2.6 langchain-core-0.2.10 langchain-text-splitters-0.2.2 langsmith-0.1.82 orjson-3.10.5 packaging-24.1 pymupdf-1.24.7 sentence-transformers-3.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport requests\nimport re\nfrom PyPDF2 import PdfMerger\nimport fitz  # PyMuPDF\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.decomposition import PCA\nimport faiss\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom transformers.utils import is_flash_attn_2_available\n\n# List of Wikipedia page titles and a direct PDF URL\nwiki_pages = [\n    \"Elections_in_India\",\n    \"2024_Indian_general_election\",\n    \"2024_elections_in_India\",\n    \"Lok_Sabha\",\n    \"Rajya_Sabha\",\n    \"State_Legislative_Assemblies\",\n    \"President_and_Vice_President\",\n    \"Election_Commission_of_India\",\n    \"2019_Indian_general_election\",\n    \"2014_Indian_general_election\",\n    \"2009_Indian_general_election\",\n    \"2004_Indian_general_election\",\n    \"1999_Indian_general_election\",\n    \"1998_Indian_general_election\",\n    \"1996_Indian_general_election\",\n    \"1991_Indian_general_election\",\n    \"1989_Indian_general_election\",\n    \"1984_Indian_general_election\",\n    \"1980_Indian_general_election\",\n    \"1977_Indian_general_election\",\n    \"1971_Indian_general_election\",\n    \"1967_Indian_general_election\",\n    \"1962_Indian_general_election\",\n    \"1957_Indian_general_election\",\n    \"1951–52_Indian_general_election\",\n    \"https://cdn.downtoearth.org.in/library/0.61706000_1558592806_first-general-elections-in-india,-vol.pdf\"\n]\n\n# Directory to save individual PDFs\npdf_dir = \"pdfs\"\nos.makedirs(pdf_dir, exist_ok=True)\n\ndef safe_filename(page):\n    \"\"\"Generate a safe filename from the page title or URL.\"\"\"\n    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", page) + \".pdf\"\n\ndef download_pdfs(wiki_pages):\n    \"\"\"Download PDFs from Wikipedia and other sources.\"\"\"\n    for page in wiki_pages:\n        pdf_path = os.path.join(pdf_dir, safe_filename(page))\n        if not os.path.exists(pdf_path):\n            print(f\"[INFO] {pdf_path} doesn't exist, downloading...\")\n            url = page if page.startswith(\"https://\") else f\"https://en.wikipedia.org/api/rest_v1/page/pdf/{page}\"\n            try:\n                response = requests.get(url)\n                response.raise_for_status()\n                with open(pdf_path, \"wb\") as file:\n                    file.write(response.content)\n                print(f\"[INFO] The file has been downloaded and saved as {pdf_path}\")\n            except requests.RequestException as e:\n                print(f\"[ERROR] Failed to download the file {page}. Error: {e}\")\n        else:\n            print(f\"[INFO] File {pdf_path} already exists.\")\n\ndef combine_pdfs(wiki_pages, combined_pdf_path):\n    \"\"\"Combine multiple PDFs into one.\"\"\"\n    pdf_merger = PdfMerger()\n    for page in wiki_pages:\n        pdf_path = os.path.join(pdf_dir, safe_filename(page))\n        if os.path.exists(pdf_path):\n            pdf_merger.append(pdf_path)\n        else:\n            print(f\"[WARN] {pdf_path} does not exist and will not be included in the combined PDF.\")\n    with open(combined_pdf_path, \"wb\") as output_file:\n        pdf_merger.write(output_file)\n    pdf_merger.close()\n    print(f\"[INFO] All PDFs have been combined into {combined_pdf_path}\")\n\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"Extract text from a PDF file.\"\"\"\n    pdf_document = fitz.open(pdf_path)\n    text = \"\"\n    for page_num in range(len(pdf_document)):\n        page = pdf_document.load_page(page_num)\n        text += page.get_text()\n    return text\n\ndef split_text_into_chunks(text, chunk_size=256, chunk_overlap=10):\n    \"\"\"Split text into smaller chunks.\"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    documents = text_splitter.create_documents([text])\n    return [doc.page_content for doc in documents]\n\ndef embed_chunks(chunks, model_name='all-MiniLM-L6-v2'):\n    \"\"\"Embed text chunks using a Sentence Transformer model.\"\"\"\n    model = SentenceTransformer(model_name)\n    return model.encode(chunks)\n\ndef build_faiss_index(embeddings):\n    \"\"\"Build a FAISS index for retrieval.\"\"\"\n    d = embeddings.shape[1]  # Dimension of embeddings\n    index = faiss.IndexFlatL2(d)  # Create a FAISS index with the correct dimension\n    index.add(embeddings)  # Add embeddings to the index\n    return index\n\ndef retrieve_similar_documents(query, index, model, chunks, k=10):\n    \"\"\"Retrieve similar documents based on a query.\"\"\"\n    query_embedding = model.encode([query])  # Encode the query\n    D, I = index.search(query_embedding, k)  # Search the index\n    similar_docs = [chunks[i] for i in I[0]]\n    distances = D[0]\n    return similar_docs, distances\n\ndef get_gpu_memory_info():\n    \"\"\"Get available GPU memory.\"\"\"\n    gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n    gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n    return gpu_memory_gb\n\ndef configure_model(gpu_memory_gb):\n    \"\"\"Configure the model based on available GPU memory.\"\"\"\n    if gpu_memory_gb < 5.1:\n        print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n    elif gpu_memory_gb < 8.1:\n        print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n        use_quantization_config = True\n        model_id = \"google/gemma-2b-it\"\n    elif gpu_memory_gb < 19.0:\n        print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n        use_quantization_config = False\n        model_id = \"google/gemma-2b-it\"\n    elif gpu_memory_gb > 19.0:\n        print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n        use_quantization_config = False\n        model_id = \"google/gemma-7b-it\"\n    print(f\"use_quantization_config set to: {use_quantization_config}\")\n    print(f\"model_id set to: {model_id}\")\n    return use_quantization_config, model_id\n\ndef setup_model(model_id, use_quantization_config):\n    os.environ['HF_TOKEN'] = \"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\"\n    \n    \"\"\"Set up the language model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id, use_auth_token=os.getenv('HF_TOKEN'))\n    quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16) if use_quantization_config else None\n    attn_implementation = \"flash_attention_2\" if (is_flash_attn_2_available() and torch.cuda.get_device_capability(0)[0] >= 8) else \"sdpa\"\n    print(f\"Using attention implementation: {attn_implementation}\")\n    llm_model = AutoModelForCausalLM.from_pretrained(\n        pretrained_model_name_or_path=model_id,\n        torch_dtype=torch.float16,\n        quantization_config=quantization_config,\n        low_cpu_mem_usage=False,\n        attn_implementation=attn_implementation,\n        use_auth_token=os.getenv('HF_TOKEN')\n    )\n    if not use_quantization_config:\n        llm_model.to(\"cuda\")\n    return tokenizer, llm_model\n\ndef format_answer(similar_docs, answer_text):\n    \"\"\"Format the answer text with context from similar documents.\"\"\"\n    formatted_answer = \"### Retrieved Documents:\\n\"\n    for i, doc in enumerate(similar_docs):\n        formatted_answer += f\"Document {i + 1}:\\n{doc}\\n\\n\"\n    formatted_answer += \"### Answer:\\n\"\n    formatted_answer += answer_text\n    return formatted_answer\n\ndef prompt_formatter(query, context_items):\n    \"\"\"\n    Format the prompt for the language model based on Indian Psephology context.\n    \"\"\"\n    # Customize the introduction based on the specific domain (Indian Psephology)\n\n    introduction = \"\"\"You are an AI model specializing in Indian elections. When answering questions, first retrieve relevant context from the provided database or source documents to ensure accurate and comprehensive responses. Only provide information and answer questions related to Indian elections, including historical election data, political parties, election processes, significant events, and notable figures. If a query is unrelated to Indian elections, politely decline to answer and remind the user to ask questions within the specified domain.\"\"\"\n\n    instructions = \"\"\"When a query is received, retrieve relevant context from the provided database or source documents.\nUse the retrieved context to generate a response.\nIf no relevant context is found, rely on your knowledge base but ensure the response is still within the scope of Indian elections.\nIf the query is unrelated to Indian elections, politely decline and remind the user to ask questions within the specified domain.\"\"\"\n\n    # Format each context item to include document numbers and titles\n    formatted_context = \"\\n\".join(context_items)\n\n    # Construct the base prompt with query and formatted context\n    base_prompt = f\"\"\"\n    User Query: {query}\n\n    {introduction}\n    {instructions}\n\n    {formatted_context}\n\n    Answer:\"\"\"\n\n    return base_prompt\n\ndef ask(query, index, model, tokenizer, llm_model, chunks, temperature=0.7, max_new_tokens=200, format_answer_text=False, return_answer_only=True):\n    \"\"\"\n    Retrieve similar documents based on the query, then generate an answer using a language model.\n    \"\"\"\n    # Retrieve similar documents\n    similar_docs, _ = retrieve_similar_documents(query, index, model, chunks)\n\n    # Format prompt\n    context_items = [f\"Document {i + 1}: {doc}\" for i, doc in enumerate(similar_docs)]\n    prompt = prompt_formatter(query, context_items)\n\n    # Generate answer using language model\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    outputs = llm_model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=temperature)\n    answer_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Optionally format the answer text with context\n    if format_answer_text:\n        answer_text = format_answer(similar_docs, answer_text)\n\n    if return_answer_only:\n        answer_text = answer_text.split(\"Answer:\\n\")[1]  # Strip the formatting to return only the answer\n\n    return answer_text\n\n# Main workflow\ndownload_pdfs(wiki_pages)\ncombined_pdf_path = \"combined_election_documents.pdf\"\ncombine_pdfs(wiki_pages, combined_pdf_path)\ntext = extract_text_from_pdf(combined_pdf_path)\nchunks = split_text_into_chunks(text)\nchunk_embeddings = embed_chunks(chunks)\n\n# Build FAISS index\nindex = build_faiss_index(chunk_embeddings)\n\n# Configure model based on GPU memory\ngpu_memory_gb = get_gpu_memory_info()\nuse_quantization_config, model_id = configure_model(gpu_memory_gb)\n\n# Set up the model\ntokenizer, llm_model = setup_model(model_id, use_quantization_config)\n\n# Initialize the SentenceTransformer model\nsentence_transformer_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T10:36:21.408635Z","iopub.execute_input":"2024-06-30T10:36:21.408975Z","iopub.status.idle":"2024-06-30T10:37:05.542603Z","shell.execute_reply.started":"2024-06-30T10:36:21.408949Z","shell.execute_reply":"2024-06-30T10:37:05.541827Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[INFO] File pdfs/Elections_in_India.pdf already exists.\n[INFO] File pdfs/2024_Indian_general_election.pdf already exists.\n[INFO] File pdfs/2024_elections_in_India.pdf already exists.\n[INFO] File pdfs/Lok_Sabha.pdf already exists.\n[INFO] File pdfs/Rajya_Sabha.pdf already exists.\n[INFO] File pdfs/State_Legislative_Assemblies.pdf already exists.\n[INFO] pdfs/President_and_Vice_President.pdf doesn't exist, downloading...\n[ERROR] Failed to download the file President_and_Vice_President. Error: 404 Client Error: Not Found for url: https://en.wikipedia.org/api/rest_v1/page/pdf/President_and_Vice_President\n[INFO] File pdfs/Election_Commission_of_India.pdf already exists.\n[INFO] File pdfs/2019_Indian_general_election.pdf already exists.\n[INFO] File pdfs/2014_Indian_general_election.pdf already exists.\n[INFO] File pdfs/2009_Indian_general_election.pdf already exists.\n[INFO] File pdfs/2004_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1999_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1998_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1996_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1991_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1989_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1984_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1980_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1977_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1971_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1967_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1962_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1957_Indian_general_election.pdf already exists.\n[INFO] File pdfs/1951–52_Indian_general_election.pdf already exists.\n[INFO] File pdfs/https___cdn.downtoearth.org.in_library_0.61706000_1558592806_first-general-elections-in-india,-vol.pdf.pdf already exists.\n[WARN] pdfs/President_and_Vice_President.pdf does not exist and will not be included in the combined PDF.\n[INFO] All PDFs have been combined into combined_election_documents.pdf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f225c8ff0a544783b28336b50fbbd73f"}},"metadata":{}},{"name":"stdout","text":"GPU memory: 15 | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\nuse_quantization_config set to: False\nmodel_id set to: google/gemma-2b-it\nUsing attention implementation: sdpa\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1540d53597914c40901782a8b7e612df"}},"metadata":{}}]},{"cell_type":"code","source":"# Ask a query\nquery = \"How to cook eggs?\"\nprint(f\"Query: {query}\")\nanswer = ask(query, index, sentence_transformer_model, tokenizer, llm_model, chunks)\nprint(f\"Answer: {answer}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T10:49:26.384480Z","iopub.execute_input":"2024-06-30T10:49:26.384846Z","iopub.status.idle":"2024-06-30T10:49:32.370007Z","shell.execute_reply.started":"2024-06-30T10:49:26.384816Z","shell.execute_reply":"2024-06-30T10:49:32.368961Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Query: How to cook eggs?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70fb696c27544b10a4b960fc60494c8c"}},"metadata":{}},{"name":"stdout","text":"Answer: \n**How to Cook Eggs in India**\n\n**Step 1: Gather Your Ingredients**\n\n* 2-3 eggs\n* 1 tablespoon of oil\n* Salt and pepper to taste\n\n**Step 2: Prepare the Eggs**\n\n* Separate the eggs into individual containers.\n* Beat the eggs with a fork or whisk until they are smooth and creamy.\n\n**Step 3: Heat the Oil**\n\n* Heat a pan or griddle over medium heat.\n* Add a few drops of oil to the pan.\n\n**Step 4: Cook the Eggs**\n\n* Pour the beaten eggs into the pan.\n* Cook for 5-7 minutes, or until the eggs are set to your desired doneness.\n* Season with salt and pepper to taste.\n\n**Step 5: Serve**\n\n* Serve the cooked eggs immediately.\n\n**Tips:**\n\n* For a firmer egg, cook for longer.\n* For a softer egg,\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split the string at \"Answer:\" and take the part that follows it\nanswer_parts = answer.split(\"Query\")\nif len(answer_parts) > 1:\n    # Take the part after \"Answer:\" and strip leading/trailing whitespace\n    answer_only = answer_parts[1].strip()\nelse:\n    # If \"Answer:\" is not found, use the whole string\n    answer_only = answer_text.strip()\n\n# Print only the answer part\nprint(\"Query\",answer_only)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T10:26:30.276423Z","iopub.execute_input":"2024-06-30T10:26:30.276759Z","iopub.status.idle":"2024-06-30T10:26:30.282690Z","shell.execute_reply.started":"2024-06-30T10:26:30.276730Z","shell.execute_reply":"2024-06-30T10:26:30.281690Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Query : Give me a brief overview of 2014 Indian general elections\nAnswer:\n\nSure, here's a brief overview of the 2014 Indian general elections:\n\n- 543 of the 545 seats in the Lok Sabha were contested in the 2014 Indian general elections.\n- The elections were held from April 12 to May 12, 2014.\n- The results were declared on May 16, 2014.\n- The Bharatiya Janata Party (BJP) emerged victorious, securing 303 seats in the Lok Sabha, the most by any party at the time.\n- The Congress Party finished second with 288 seats, while the Indian National Congress (INC) secured 108 seats.\n- The BJP's victory marked a significant shift in the political landscape of India, as it had won a majority of the Lok Sabha seats for the first time since 2004.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}